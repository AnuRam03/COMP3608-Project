{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507a91e0",
   "metadata": {},
   "source": [
    "## Algorithm Selection for Credit Card Fraud Detection\n",
    "\n",
    "For this project, which focuses on detecting credit card fraud, we have selected three classification algorithms: Logistic Regression, Random Forest, and Linear Support Vector Machine (Linear SVC). The choice is driven by the need to explore different approaches to a common, imbalanced classification problem.\n",
    "\n",
    "Credit card fraud detection typically involves:\n",
    "*   **Highly Imbalanced Data:** Legitimate transactions vastly outnumber fraudulent ones.\n",
    "*   **Complex Patterns:** Fraudulent activities can manifest in subtle and non-linear ways.\n",
    "*   **Interpretability vs. Performance Trade-off:** While high accuracy is crucial, understanding model decisions can also be valuable.\n",
    "*   **Feature Engineering:** The raw transaction data often benefits from feature engineering to create more informative predictors.\n",
    "\n",
    "Our chosen algorithms allow us to compare models with varying complexities, assumptions, and performance characteristics:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Logistic Regression\n",
    "\n",
    "**Brief Description:** A linear model that predicts the probability of a binary outcome (fraud/no fraud) using a sigmoid function applied to a linear combination of input features.\n",
    "\n",
    "**Why Chosen for Credit Card Fraud Detection:**\n",
    "\n",
    "*   Excellent Baseline: Provides a strong, interpretable baseline to compare against more complex models. Its simplicity makes it computationally efficient and easy to understand.\n",
    "*   Probabilistic Output: Directly outputs probabilities of fraud, which is highly valuable. This allows for:\n",
    "    *   Fine-tuning the decision threshold based on business needs (e.g., balancing the cost of false positives vs. false negatives).\n",
    "    *   Ranking transactions by risk.\n",
    "*   Interpretability: The coefficients of a trained logistic regression model can offer insights into the direction and strength of association between features and the likelihood of fraud (though this becomes more complex with many features or interactions).\n",
    "*   Addresses Linearity (as a starting point): While fraud patterns can be non-linear, logistic regression helps establish if linear relationships alone can capture a significant portion of the fraudulent behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Random Forest\n",
    "\n",
    "**Brief Description:** An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) of the individual trees.\n",
    "\n",
    "**Why Chosen for Credit Card Fraud Detection:**\n",
    "\n",
    "*   High Predictive Power & Robustness: Random Forests are known for their high accuracy and robustness. They can capture complex, non-linear relationships in the data that linear models might miss.\n",
    "*   Handles Imbalanced Data Relatively Well: While specific techniques (like SMOTE or class weighting) can further improve performance on imbalanced datasets, Random Forests are generally less sensitive to class imbalance than some other algorithms due to their ensemble nature and tree-building process.\n",
    "*   Feature Importance: Provides a built-in mechanism to estimate the importance of each feature in predicting fraud. This is invaluable for understanding which factors are most indicative of fraudulent activity and can guide future feature engineering or data collection.\n",
    "*   Reduced Overfitting: By averaging the predictions of many decorrelated trees (due to bootstrap sampling and random feature subspace selection), it's less prone to overfitting than a single decision tree.\n",
    "*   No Need for Extensive Feature Scaling: Tree-based models are generally insensitive to the scale of features.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Linear Support Vector Machine (Linear SVC)\n",
    "\n",
    "**Brief Description:** A linear classifier that aims to find the hyperplane that best separates two classes by maximizing the margin (the distance between the hyperplane and the closest data points from each class, known as support vectors).\n",
    "\n",
    "**Why Chosen for Credit Card Fraud Detection:**\n",
    "\n",
    "*   Effective in High-Dimensional Spaces: Credit card transaction data can have many features. SVMs, including Linear SVC, can perform well even when the number of features is large.\n",
    "*   Maximizes Margin for Separation: The core idea of maximizing the margin often leads to good generalization performance, which is crucial for unseen data.\n",
    "*   Different Decision Boundary Philosophy: Unlike probabilistic models (Logistic Regression) or ensemble tree-based models (Random Forest), SVMs focus on finding an optimal separating boundary. This provides a different perspective on the classification task.\n",
    "*   Regularization via `C` Parameter: The `C` parameter controls the trade-off between maximizing the margin and minimizing misclassifications, allowing for tuning to prevent overfitting and handle noisy data.\n",
    "*   Efficiency with Linear Kernel: For large datasets, Linear SVC can be computationally more efficient than SVMs with non-linear kernels while still providing strong performance if the problem has some linear separability.\n",
    "*   Consideration for Scaled Data: While a potential challenge, it forces consideration of data preprocessing (feature scaling is crucial for SVMs), which is good practice for many algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54d32a",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression predicts the probability of a binary outcome using a **sigmoid** function. The model assumes the log‑odds (logit) of fraud is a linear combination of features:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\log\\!\\biggl(\\frac{P(\\text{fraud}=1 \\mid \\mathbf{x})}{1 - P(\\text{fraud}=1 \\mid \\mathbf{x})}\\biggr)\n",
    "= \\mathbf{w}^T \\mathbf{x} + c\n",
    "$$\n",
    "\n",
    "Solving for the probability:\n",
    "\n",
    "$$\n",
    "P(\\text{fraud}=1 \\mid \\mathbf{x})\n",
    "= \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + c)}}\n",
    "$$\n",
    "\n",
    "**Optimization:** Minimize log loss (negative log‑likelihood):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, c)\n",
    "= -\\frac{1}{n} \\sum_{i=1}^n \\Bigl[y_i \\log(p_i) + (1 - y_i)\\log(1 - p_i)\\Bigr]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p_i = \\sigma(\\mathbf{w}^T \\mathbf{x}_i + c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### From the log‑odds (logit) to the sigmoid\n",
    "\n",
    "1. Intial Form \n",
    "   $$\n",
    "   \\log\\!\\biggl(\\frac{p}{1 - p}\\biggr)\n",
    "   = \\mathbf{w}^T \\mathbf{x} + c,\n",
    "   \\quad\\text{where }p = P(fraud = 1 \\mid \\mathbf{x})\n",
    "   $$\n",
    "\n",
    "2. Exponentiate both sides:\n",
    "   $$\n",
    "   \\frac{p}{1 - p}\n",
    "   = e^{\\,\\mathbf{w}^T \\mathbf{x} + c}.\n",
    "   $$\n",
    "\n",
    "3. Solve for \\(p\\):\n",
    "\n",
    "   $$\n",
    "   p = (1 - p)\\,e^{\\,\\mathbf{w}^T \\mathbf{x} + c} \n",
    "   \\implies p + p\\,e^{\\,\\mathbf{w}^T \\mathbf{x} + c} = e^{\\,\\mathbf{w}^T \\mathbf{x} + c} \n",
    "   \\implies p\\bigl(1 + e^{\\,\\mathbf{w}^T \\mathbf{x} + c}\\bigr) = e^{\\,\\mathbf{w}^T \\mathbf{x} + c}\n",
    "   \\implies p = \\frac{e^{\\,\\mathbf{w}^T \\mathbf{x} + c}}{1 + e^{\\,\\mathbf{w}^T \\mathbf{x} + c}} = \\frac{1}{1 + e^{-\\,(\\mathbf{w}^T \\mathbf{x} + c)}} = \\sigma(\\mathbf{w}^T \\mathbf{x} + c).\n",
    "   $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### From the Bernoulli likelihood to the negative log‑likelihood (NLL)\n",
    "Since our labels are binary, we model each $y_i$ with a Bernoulli distribution:\n",
    "\n",
    "$$\n",
    "P(y_i \\mid \\mathbf{x}_i)\n",
    "= p_i^{y_i}\\,(1 - p_i)^{1 - y_i},\n",
    "\\qquad\n",
    "p_i = \\sigma\\bigl(\\mathbf{w}^\\top \\mathbf{x}_i + b\\bigr),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ are the weights, and $b$ is the bias (intercept).\n",
    "\n",
    "For the entire dataset $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$, the joint likelihood is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w},b)\n",
    "= \\prod_{i=1}^n P(y_i \\mid \\mathbf{x}_i)\n",
    "= \\prod_{i=1}^n\n",
    "  \\bigl[p_i\\bigr]^{y_i}\n",
    "  \\bigl[1 - p_i\\bigr]^{1 - y_i}.\n",
    "$$\n",
    "\n",
    "Because products can underflow, we take the (monotonic) logarithm to obtain the log-likelihood, converting products into sums.\n",
    "A monotonic transformation will change a set of number values but keep their ordering, meaning that the biggest value before the transfromation will also be the biggest value after the transformation.\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{w},b)\n",
    "= \\log \\mathcal{L}(\\mathbf{w},b)\n",
    "= \\sum_{i=1}^n \\Bigl[\n",
    "    y_i\\,\\log p_i + (1 - y_i)\\,\\log(1 - p_i)\n",
    "  \\Bigr].\n",
    "$$\n",
    "\n",
    "Finally, we multiply by $-\\frac{1}{n}$ so that maximizing the log-likelihood becomes an equivalent **minimization** problem (the average “log-loss” or negative log-likelihood):\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\mathbf{w},b)\n",
    "= -\\frac{1}{n}\\,\\ell(\\mathbf{w},b)\n",
    "= -\\frac{1}{n}\\sum_{i=1}^n\n",
    "  \\Bigl[\n",
    "    y_i \\,\\log \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\n",
    "    + (1 - y_i)\\,\\log\\bigl(1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\bigr)\n",
    "  \\Bigr].\n",
    "$$\n",
    "\n",
    "\n",
    "The joint likelihood can be transformed into a log-likelihood because the actual values do not matter, only the order matters as it's used for a maximization problem. The log-likelihood is then multiplied by -1/n to turn it into a minimization problem so conventional optimization solutions can be used.\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths:\n",
    "\n",
    " - Outputs calibrated probabilities for interpretable decision-making.\n",
    "\n",
    " - Efficient training with convex optimization (guaranteed global minimum).\n",
    "\n",
    " - L2 regularization (default in scikit-learn) mitigates overfitting.\n",
    "\n",
    "### Weaknesses:\n",
    "\n",
    " - Assumes linear decision boundaries; struggles with non-linear patterns.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433901a",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It corrects for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "---\n",
    "\n",
    "### How it Works (Classification)\n",
    "\n",
    "#### 1. Bootstrap Sampling (Bagging)\n",
    "\n",
    "From the original dataset of $n$ samples, $N$ bootstrap samples are drawn **with replacement**.  \n",
    "This means each new sample is the same size as the original, but some original data points may appear multiple times, and others not at all (these are \"out-of-bag\" samples).\n",
    "\n",
    "#### 2. Random Feature Selection\n",
    "\n",
    "For each bootstrap sample, a decision tree is grown. However, at each node, when deciding on the best split, only a random subset of $m$ features is considered, where $m < M$ (with $M$ being the total number of features).  \n",
    "A common choice for $m$ in classification tasks is:  \n",
    "$$\n",
    "m = \\sqrt{M}\n",
    "$$\n",
    "\n",
    "#### 3. Tree Growth\n",
    "\n",
    "Each tree is grown to its maximum depth (or until a pre-defined limit is reached, or nodes contain a minimum number of samples), typically **without pruning**.  \n",
    "The best split at each node is chosen based on a criterion like **Gini impurity** or **information gain**, but only among the $m$ randomly selected features.\n",
    "\n",
    "#### 4. Aggregation\n",
    "\n",
    "For a new input instance, it is passed down all $N$ trees in the forest. Each tree gives a classification (a \"vote\").  \n",
    "The forest chooses the classification having the **most votes** among all the trees.  \n",
    "To get probability estimates, use the proportion of votes each class receives.\n",
    "\n",
    "---\n",
    "\n",
    "### Optimization (Implicit)\n",
    "\n",
    "Random Forest does not optimize a single global loss function (unlike logistic regression or SVM). Instead, optimization occurs at two levels:\n",
    "\n",
    "#### 1. Individual Tree Optimization\n",
    "\n",
    "Each decision tree chooses splits that **minimize impurity** at each node.\n",
    "\n",
    "**Gini Impurity** is used by the selected model from sklearn. For a node $t$ with $K$ classes, and $p(j \\mid t)$ being the proportion of samples of class $j$ at node $t$, the Gini impurity is:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(t) = 1 - \\sum_{j=1}^{K} \\left[p(j \\mid t)\\right]^2\n",
    "$$\n",
    "\n",
    "The algorithm prefers splits that result in the **greatest reduction** in impurity.\n",
    "\n",
    "#### 2. Ensemble Strategy\n",
    "\n",
    "Random Forest improves performance by:\n",
    "\n",
    "- **Averaging out the variance** of individual trees (which are high-variance models),\n",
    "- **Reducing correlation** between trees through bootstrap sampling and random feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- High Accuracy: Generally performs very well on a wide range of classification (and regression) problems.  \n",
    "- Robust to Overfitting: Due to averaging predictions from multiple decorrelated trees, it's less prone to overfitting than a single decision tree.  \n",
    "- Handles Non-linear Data: Can capture complex, non-linear relationships between features and the target.    \n",
    "- Handles Missing Values & Outliers: Can handle missing data (e.g., by surrogate splits or imputation) and is relatively robust to outliers.  \n",
    "- No Need for Feature Scaling: Tree-based methods are not sensitive to the scale of input features.  \n",
    "\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- **Less Interpretable**: Can be seen as a \"black box\" model. Understanding the exact reasoning behind a prediction is difficult compared to a single decision tree or a linear model.  \n",
    "- Computationally Intensive: Training many trees can be computationally expensive and require more memory, especially with large datasets and many trees.  \n",
    "- May Not Perform Well on Very Sparse Data: For very high-dimensional, sparse data (e.g., text data), linear models might perform better.  \n",
    "- Can Still Overfit on Noisy Data: If the trees are too deep and the dataset is very noisy, it can still overfit, though less so than individual trees. Careful hyperparameter tuning (e.g., `max_depth`, `min_samples_split`) is important.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b26d35",
   "metadata": {},
   "source": [
    "\n",
    "## Linear SVM (Support Vector Machine)\n",
    "\n",
    "Linear SVM aims to find a hyperplane that maximally separates two classes. It does this by maximizing the margin between the closest points of each class (support vectors) and the decision boundary.\n",
    "\n",
    "### Decision Function\n",
    "\n",
    "The decision function for a linear SVM is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "The predicted label is:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } f(\\mathbf{x}) \\geq 0 \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Objective: Maximize the Margin\n",
    "\n",
    "The margin is defined as the distance between the hyperplane and the nearest data points. For linearly separable data, the margin is:\n",
    "\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "So we minimize $ \\|\\mathbf{w}\\|^2 $ to maximize the margin.\n",
    "\n",
    "### Primal Optimization Problem\n",
    "\n",
    "For soft‑margin SVM, which allows some misclassification (via slack variables $ \\xi_i $), the optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w},\\, b,\\, \\boldsymbol{\\xi}} \\quad & \\frac{1}{2}\\,\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\\\\n",
    "\\text{subject to} \\quad & y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0,\\quad i = 1, \\dots, n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* $ C > 0 $ is a **regularization hyperparameter** that controls the trade‑off between maximizing the margin and minimizing the classification error  \n",
    "  * **Small $C$**: Wider margin, more misclassifications allowed (more regularization, higher bias, lower variance).  \n",
    "  * **Large $C$**: Narrower margin, fewer misclassifications allowed (less regularization, lower bias, higher variance).  \n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "The hinge loss penalizes misclassified points and points within the margin:\n",
    "\n",
    "$$\n",
    "\\text{Hinge}(y_i, f(\\mathbf{x}_i)) = \\max\\bigl(0,\\; 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\bigr)\n",
    "$$\n",
    "\n",
    "The equivalent loss function is therefore:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w},\\, b} \\quad \\frac{1}{2}\\,\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\max\\bigl(0,\\; 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\bigr)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- **Effective in high‑dimensional spaces**.  \n",
    "- Works well when the number of features is greater than the number of samples.  \n",
    "- Uses only **support vectors**, which makes the model memory‑efficient.  \n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Sensitive to the choice of the regularization parameter $C$.  \n",
    "- Output is not probabilistic (though it can be calibrated).  \n",
    "- Struggles with non‑linearly separable data without kernel tricks.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
