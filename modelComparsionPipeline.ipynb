{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f441e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\Documents\\COMP3608-Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\loren\\Documents\\COMP3608-Project\"\n",
    "import pandas as pd\n",
    "\n",
    "#Algorithms used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Scalers to test\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Sampling techniques to test\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from HelpFunctions.comparison_pipline import model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c4a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model, resampling techniques and sampling methods to be compared are all added to a dict\n",
    "models_to_test = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Linear SVC': LinearSVC(dual=False, random_state=42, max_iter=2000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b952698",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scalers = {\n",
    "        'None': None,\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10daf42d",
   "metadata": {},
   "source": [
    "## Data Scaling Techniques\n",
    "\n",
    "Feature scaling is a crucial preprocessing step for many machine learning algorithms, especially those that are sensitive to the magnitude of input features. Without scaling, features with larger values might disproportionately influence the model, leading to suboptimal performance. We will explore the following scaling techniques:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `None`: No Scaler (Baseline)\n",
    "\n",
    "*   **Purpose:**\n",
    "    This approach serves as a **baseline** to understand the impact of feature scaling. By training models on unscaled data, we can observe if and how much the chosen algorithms are affected by differences in feature magnitudes. This helps to justify the use of scaling if scaled versions perform better.\n",
    "\n",
    "*   **Applicability & Considerations:**\n",
    "    *   **Tree-based models** (like Random Forest and Decision Trees) are generally insensitive to feature scaling because their splitting decisions are based on single features and thresholds, not on distances or magnitudes across features. For these models, `None` might yield similar performance to scaled versions.\n",
    "    *   **Distance-based algorithms** (like K-Nearest Neighbors, SVMs) and **gradient-based algorithms** (like Logistic Regression, Neural Networks, or anything optimized with gradient descent) are highly sensitive to feature scaling. Without scaling, features with larger values and wider ranges can dominate the distance calculations or the gradient updates, leading to slower convergence or a suboptimal solution.\n",
    "    *   For algorithms like **Linear SVM (LinearSVC)** and **Logistic Regression**, using unscaled data when features have vastly different ranges is generally not recommended.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `StandardScaler`: Standardization\n",
    "\n",
    "*   **How it Works:**\n",
    "    `StandardScaler` transforms the data by removing the mean and scaling to unit variance. For each feature $x$, the standardized value $x_{scaled}$ is calculated as:\n",
    "    $$\n",
    "    x_{scaled} = \\frac{x - \\mu}{\\sigma}\n",
    "    $$\n",
    "    where:\n",
    "    *   $\\mu$ is the mean of the feature in the training set.\n",
    "    *   $\\sigma$ is the standard deviation of the feature in the training set.\n",
    "\n",
    "    The result is that each feature will have a mean of 0 and a standard deviation of 1 after transformation.\n",
    "\n",
    "*   **Purpose:**\n",
    "    To center the data around zero and give all features a similar scale (unit variance). This is particularly important for algorithms that assume data is centered around zero or that features have similar variance, such as:\n",
    "    *   **Support Vector Machines (SVMs / LinearSVC):** The optimization process for finding the maximal margin hyperplane is sensitive to feature scales.\n",
    "    *   **Logistic Regression (and other models using gradient descent):** Standardization can help gradient descent converge faster and more reliably.\n",
    "    *   Principal Component Analysis (PCA).\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Maintains the shape of the original distribution (does not make it Gaussian if it wasn't).\n",
    "    *   Less affected by outliers compared to `MinMaxScaler` because it uses mean and standard deviation, which are influenced by outliers, but the scaling itself doesn't compress values into a strict range determined by min/max.\n",
    "    *   Often a good default choice for many algorithms.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   The transformed features do not have a strictly bounded range (e.g., between 0 and 1), which might be desirable for some specific algorithms or interpretations, though less common.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `MinMaxScaler`: Normalization (Min-Max Scaling)\n",
    "\n",
    "*   **How it Works:**\n",
    "    `MinMaxScaler` transforms features by scaling each feature to a given range, typically between 0 and 1 (which is the default). For each feature $x$, the scaled value $x_{scaled}$ is calculated as:\n",
    "    $$\n",
    "    x_{scaled} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "    $$\n",
    "    where:\n",
    "    *   $\\min(x)$ is the minimum value of the feature in the training set.\n",
    "    *   $\\max(x)$ is the maximum value of the feature in the training set.\n",
    "\n",
    "    If a specific range `(a, b)` is desired instead of `(0, 1)`, the formula becomes:\n",
    "    $$\n",
    "    x_{scaled} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\times (b - a) + a\n",
    "    $$\n",
    "\n",
    "*   **Purpose:**\n",
    "    To scale all features into a fixed range, usually [0, 1]. This can be beneficial for algorithms that:\n",
    "    *   Expect inputs within a specific bounded range (e.g., some neural network activation functions historically preferred inputs in [0,1] or [-1,1]).\n",
    "    *   Are sensitive to feature magnitudes, similar to `StandardScaler`. For algorithms like SVMs and Logistic Regression, `MinMaxScaler` can also be effective.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Guarantees that all features will have the exact same scale (e.g., [0, 1]), which can be useful if strict bounds are required.\n",
    "    *   Can be good when the data distribution is not Gaussian and the algorithm does not assume any specific distribution.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Highly sensitive to outliers:** If there are very large or very small outliers in a feature, they will become the new `min(x)` or `max(x)`. This can cause the majority of the \"normal\" data points to be compressed into a very small sub-interval of the [0, 1] range, potentially diminishing the variance and discriminative power of that feature for the bulk of the data.\n",
    "    *   May compress the data too much if the original standard deviation is small, potentially losing some information about variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4672961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samplers = {\n",
    "    'None': None,\n",
    "    'SMOTE': SMOTE(random_state=42), \n",
    "    'RUS': RandomUnderSampler(random_state=42), \n",
    "    'ROS': RandomOverSampler(random_state=42)   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6d01c",
   "metadata": {},
   "source": [
    "## Data Sampling Techniques\n",
    "\n",
    "From the EDAs done we saw that credit card fraud is highly imbalanced for dataset 2 and dataset 3, with a very small percentage of transactions being fraudulent (the minority class) compared to legitimate ones (the majority class). Training models directly on such imbalanced data can lead to classifiers that are biased towards the majority class, performing poorly in identifying fraud. To address this, the following sampling techniques will be explored\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `None`: No Sampling (Baseline)\n",
    "\n",
    "*   **Purpose:**\n",
    "    This approach serves as a **baseline**. By training models on the raw, imbalanced data, we can establish a performance benchmark. The results from this baseline will help quantify the actual impact and benefit of applying different sampling techniques.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Represents the true underlying distribution of the data.\n",
    "    *   No artificial data is introduced, and no original data points are discarded at the sampling stage.\n",
    "    *   Allows evaluation of how well a model can inherently handle imbalance.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   Models are likely to be biased towards the majority class (legitimate transactions).\n",
    "    *   May result in poor recall for the minority class (fraudulent transactions), meaning many fraud cases might be missed.\n",
    "    *   Metrics like accuracy can be misleading, as a model predicting everything as \"legitimate\" would achieve high accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `SMOTE`: Synthetic Minority Over-sampling Technique\n",
    "\n",
    "*   **How it Works:**\n",
    "    SMOTE is an over-sampling technique that creates **synthetic** samples for the minority class (fraudulent transactions) rather than just duplicating existing ones. For each minority class sample:\n",
    "    1.  It finds its *k*-nearest neighbors (also from the minority class).\n",
    "    2.  It randomly selects one or more of these neighbors.\n",
    "    3.  New synthetic samples are generated along the line segment joining the original minority sample and its selected neighbor(s) in the feature space.\n",
    "    The `SMOTE(random_state=42)` ensures that the random choices made during this process (like selecting neighbors or the point along the line segment) are reproducible.\n",
    "\n",
    "*   **Purpose:**\n",
    "    To increase the representation of the minority class by generating new, plausible minority samples, thereby helping to balance the class distribution and provide more \"examples\" for the model to learn the characteristics of the minority class.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Can lead to better generalization for the minority class by creating a more diverse set of minority samples compared to simple over-sampling.\n",
    "    *   Helps to make the decision regions for the minority class less specific to the exact original minority samples, potentially reducing overfitting that ROS might cause.\n",
    "    *   Provides more robust \"signals\" for identifying fraudulent patterns.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   Can create noisy samples if the original minority samples are themselves noisy or if the synthetic samples are generated in regions that overlap significantly with the majority class, potentially blurring decision boundaries.\n",
    "    *   Does not consider the majority class when generating samples, which might lead to the creation of synthetic samples in areas of high majority class density.\n",
    "    *   Can be computationally more intensive than simpler sampling methods.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `RUS`: Random Under-Sampler\n",
    "\n",
    "*   **How it Works:**\n",
    "    RUS aims to balance class distribution by randomly removing samples from the **majority class** (legitimate transactions). It continues to discard majority class samples until the desired ratio between minority and majority class instances is achieved (often aiming for a 1:1 ratio, or a predefined sampling strategy).\n",
    "    The `RandomUnderSampler(random_state=42)` ensures that the random selection of majority class samples to be discarded is reproducible.\n",
    "\n",
    "*   **Purpose:**\n",
    "    To reduce the skewness in the dataset by decreasing the number of majority class samples, making the dataset more balanced and potentially reducing the computational burden of training.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Can significantly reduce the size of the training dataset, leading to faster model training times.\n",
    "    *   Can help prevent models from being overwhelmed by the sheer volume of legitimate transactions, allowing them to pay more \"attention\" to the fraudulent ones.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Potential loss of important information:** By randomly discarding majority class samples, we might remove legitimate transactions that are crucial for defining the decision boundary between fraudulent and non-fraudulent behavior (e.g., legitimate transactions that look somewhat similar to fraudulent ones).\n",
    "    *   May not be suitable if the original dataset size is small, as further reducing it could lead to insufficient data for robust model training.\n",
    "    *   The random nature means that different runs (without a fixed `random_state`) could lead to different subsets and potentially different model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `ROS`: Random Over-Sampler\n",
    "\n",
    "*   **How it Works:**\n",
    "    ROS aims to balance class distribution by randomly duplicating samples from the **minority class** (fraudulent transactions). Existing minority class samples are selected at random (with replacement) and added to the dataset until the desired ratio between minority and majority class instances is achieved.\n",
    "    The `RandomOverSampler(random_state=42)` ensures that the random selection of minority class samples for duplication is reproducible.\n",
    "\n",
    "*   **Purpose:**\n",
    "    To increase the representation of the minority class by increasing its sample size, thereby balancing the dataset.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   Simple to understand and implement.\n",
    "    *   No information from the original dataset is lost (unlike RUS).\n",
    "    *   Can sometimes be effective for algorithms that are sensitive to class distribution.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Prone to overfitting:** Since it merely makes exact copies of existing minority samples, the model might learn these specific instances too well without generalizing to new, unseen fraudulent transactions. The model might become too specific to the duplicated patterns.\n",
    "    *   Does not add any genuinely \"new\" information or variability to the minority class.\n",
    "    *   Can significantly increase the size of the training dataset, potentially increasing training time, though usually less of a concern than the overfitting risk for fraud data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15f125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset3\n",
    "df = pd.read_csv('card_transdata.csv', index_col=0)\n",
    "binary_columns = {'repeat_retailer': 'bool', 'used_chip': 'bool',\n",
    "                  'used_pin_number': 'bool', 'online_order': 'bool', 'fraud': 'bool'}\n",
    "df = df.astype(binary_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ef2dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numerical columns for scaling: ['distance_from_last_transaction', 'ratio_to_median_purchase_price']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"fraud\"])\n",
    "y = df[\"fraud\"]\n",
    "\n",
    "numerical_cols_actual = [col for col in X.columns if X[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "print(f\"Using numerical columns for scaling: {numerical_cols_actual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f7505bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: test_size=0.2, random_state=42, stratify=y\n",
      "Train shape: (800000, 6), Test shape: (200000, 6)\n",
      "Test label distribution:\n",
      "fraud\n",
      "False    0.912595\n",
      "True     0.087405\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "\n",
      "=== Evaluating Model: Logistic Regression ===\n",
      "\n",
      "=== Evaluating Model: Random Forest ===\n",
      "\n",
      "=== Evaluating Model: Linear SVC ===\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "\n",
      "=== Pipeline Execution Complete ===\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "all_results = model_comparison(models_to_test, scalers, samplers, X, y, numerical_cols_actual, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9335c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.981155</td>\n",
       "      <td>0.997749</td>\n",
       "      <td>0.786168</td>\n",
       "      <td>0.879411</td>\n",
       "      <td>0.958454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.980790</td>\n",
       "      <td>0.992347</td>\n",
       "      <td>0.786282</td>\n",
       "      <td>0.877378</td>\n",
       "      <td>0.946634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.980780</td>\n",
       "      <td>0.991849</td>\n",
       "      <td>0.786568</td>\n",
       "      <td>0.877361</td>\n",
       "      <td>0.945886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.978430</td>\n",
       "      <td>0.957156</td>\n",
       "      <td>0.788513</td>\n",
       "      <td>0.864689</td>\n",
       "      <td>0.958992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.976125</td>\n",
       "      <td>0.925976</td>\n",
       "      <td>0.790001</td>\n",
       "      <td>0.852601</td>\n",
       "      <td>0.947014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.976010</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>0.789943</td>\n",
       "      <td>0.851987</td>\n",
       "      <td>0.947308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.952125</td>\n",
       "      <td>0.694155</td>\n",
       "      <td>0.808478</td>\n",
       "      <td>0.746968</td>\n",
       "      <td>0.962865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.931895</td>\n",
       "      <td>0.577604</td>\n",
       "      <td>0.821749</td>\n",
       "      <td>0.678378</td>\n",
       "      <td>0.961545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.931165</td>\n",
       "      <td>0.574144</td>\n",
       "      <td>0.822607</td>\n",
       "      <td>0.676276</td>\n",
       "      <td>0.962301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.954775</td>\n",
       "      <td>0.921042</td>\n",
       "      <td>0.527830</td>\n",
       "      <td>0.671079</td>\n",
       "      <td>0.928591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.954755</td>\n",
       "      <td>0.920927</td>\n",
       "      <td>0.527659</td>\n",
       "      <td>0.670910</td>\n",
       "      <td>0.928608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.913450</td>\n",
       "      <td>0.502949</td>\n",
       "      <td>0.834048</td>\n",
       "      <td>0.627502</td>\n",
       "      <td>0.963127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.901355</td>\n",
       "      <td>0.464576</td>\n",
       "      <td>0.843258</td>\n",
       "      <td>0.599094</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.901045</td>\n",
       "      <td>0.463627</td>\n",
       "      <td>0.842172</td>\n",
       "      <td>0.598030</td>\n",
       "      <td>0.962914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.423494</td>\n",
       "      <td>0.819976</td>\n",
       "      <td>0.558526</td>\n",
       "      <td>0.937437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.886695</td>\n",
       "      <td>0.423481</td>\n",
       "      <td>0.819976</td>\n",
       "      <td>0.558515</td>\n",
       "      <td>0.937440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.886530</td>\n",
       "      <td>0.423046</td>\n",
       "      <td>0.819690</td>\n",
       "      <td>0.558070</td>\n",
       "      <td>0.937440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.886455</td>\n",
       "      <td>0.422768</td>\n",
       "      <td>0.818546</td>\n",
       "      <td>0.557562</td>\n",
       "      <td>0.937068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.886045</td>\n",
       "      <td>0.421755</td>\n",
       "      <td>0.818660</td>\n",
       "      <td>0.556707</td>\n",
       "      <td>0.937122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.886045</td>\n",
       "      <td>0.421751</td>\n",
       "      <td>0.818603</td>\n",
       "      <td>0.556690</td>\n",
       "      <td>0.937123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.885770</td>\n",
       "      <td>0.421063</td>\n",
       "      <td>0.818546</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>0.937128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.885620</td>\n",
       "      <td>0.420701</td>\n",
       "      <td>0.818660</td>\n",
       "      <td>0.555789</td>\n",
       "      <td>0.937150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.882435</td>\n",
       "      <td>0.413735</td>\n",
       "      <td>0.827470</td>\n",
       "      <td>0.551647</td>\n",
       "      <td>0.938846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.413657</td>\n",
       "      <td>0.827527</td>\n",
       "      <td>0.551590</td>\n",
       "      <td>0.938859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.881915</td>\n",
       "      <td>0.412558</td>\n",
       "      <td>0.828042</td>\n",
       "      <td>0.550726</td>\n",
       "      <td>0.938884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.881850</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.827985</td>\n",
       "      <td>0.550572</td>\n",
       "      <td>0.938878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.881790</td>\n",
       "      <td>0.412274</td>\n",
       "      <td>0.828156</td>\n",
       "      <td>0.550498</td>\n",
       "      <td>0.938881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.881775</td>\n",
       "      <td>0.412159</td>\n",
       "      <td>0.827241</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>0.938625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.878735</td>\n",
       "      <td>0.403665</td>\n",
       "      <td>0.811624</td>\n",
       "      <td>0.539170</td>\n",
       "      <td>0.934627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.942765</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>0.374807</td>\n",
       "      <td>0.533746</td>\n",
       "      <td>0.918991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.875065</td>\n",
       "      <td>0.395617</td>\n",
       "      <td>0.813683</td>\n",
       "      <td>0.532385</td>\n",
       "      <td>0.934880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.874865</td>\n",
       "      <td>0.395194</td>\n",
       "      <td>0.813855</td>\n",
       "      <td>0.532039</td>\n",
       "      <td>0.934895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.938895</td>\n",
       "      <td>0.911066</td>\n",
       "      <td>0.333448</td>\n",
       "      <td>0.488211</td>\n",
       "      <td>0.924636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.938895</td>\n",
       "      <td>0.911066</td>\n",
       "      <td>0.333448</td>\n",
       "      <td>0.488211</td>\n",
       "      <td>0.924636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.938065</td>\n",
       "      <td>0.909749</td>\n",
       "      <td>0.323494</td>\n",
       "      <td>0.477276</td>\n",
       "      <td>0.924119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.335372</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.475316</td>\n",
       "      <td>0.924706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model          Scaler Sampler  Accuracy  Precision  \\\n",
       "0         Random Forest    MinMaxScaler    None  0.981155   0.997749   \n",
       "1         Random Forest            None    None  0.980790   0.992347   \n",
       "2         Random Forest  StandardScaler    None  0.980780   0.991849   \n",
       "3         Random Forest    MinMaxScaler     ROS  0.978430   0.957156   \n",
       "4         Random Forest            None     ROS  0.976125   0.925976   \n",
       "5         Random Forest  StandardScaler     ROS  0.976010   0.924607   \n",
       "6         Random Forest    MinMaxScaler   SMOTE  0.952125   0.694155   \n",
       "7         Random Forest            None   SMOTE  0.931895   0.577604   \n",
       "8         Random Forest  StandardScaler   SMOTE  0.931165   0.574144   \n",
       "9   Logistic Regression            None    None  0.954775   0.921042   \n",
       "10  Logistic Regression  StandardScaler    None  0.954755   0.920927   \n",
       "11        Random Forest    MinMaxScaler     RUS  0.913450   0.502949   \n",
       "12        Random Forest  StandardScaler     RUS  0.901355   0.464576   \n",
       "13        Random Forest            None     RUS  0.901045   0.463627   \n",
       "14           Linear SVC  StandardScaler     ROS  0.886700   0.423494   \n",
       "15           Linear SVC            None     ROS  0.886695   0.423481   \n",
       "16           Linear SVC  StandardScaler   SMOTE  0.886530   0.423046   \n",
       "17           Linear SVC            None   SMOTE  0.886455   0.422768   \n",
       "18           Linear SVC  StandardScaler     RUS  0.886045   0.421755   \n",
       "19           Linear SVC            None     RUS  0.886045   0.421751   \n",
       "20           Linear SVC    MinMaxScaler     ROS  0.885770   0.421063   \n",
       "21           Linear SVC    MinMaxScaler   SMOTE  0.885620   0.420701   \n",
       "22  Logistic Regression            None     RUS  0.882435   0.413735   \n",
       "23  Logistic Regression  StandardScaler     RUS  0.882400   0.413657   \n",
       "24  Logistic Regression            None     ROS  0.881915   0.412558   \n",
       "25  Logistic Regression  StandardScaler     ROS  0.881850   0.412400   \n",
       "26  Logistic Regression  StandardScaler   SMOTE  0.881790   0.412274   \n",
       "27  Logistic Regression            None   SMOTE  0.881775   0.412159   \n",
       "28           Linear SVC    MinMaxScaler     RUS  0.878735   0.403665   \n",
       "29  Logistic Regression    MinMaxScaler    None  0.942765   0.926733   \n",
       "30  Logistic Regression    MinMaxScaler     ROS  0.875065   0.395617   \n",
       "31  Logistic Regression    MinMaxScaler   SMOTE  0.874865   0.395194   \n",
       "32           Linear SVC  StandardScaler    None  0.938895   0.911066   \n",
       "33           Linear SVC            None    None  0.938895   0.911066   \n",
       "34           Linear SVC    MinMaxScaler    None  0.938065   0.909749   \n",
       "35  Logistic Regression    MinMaxScaler     RUS  0.842600   0.335372   \n",
       "\n",
       "      Recall  F1 Score   ROC AUC  \n",
       "0   0.786168  0.879411  0.958454  \n",
       "1   0.786282  0.877378  0.946634  \n",
       "2   0.786568  0.877361  0.945886  \n",
       "3   0.788513  0.864689  0.958992  \n",
       "4   0.790001  0.852601  0.947014  \n",
       "5   0.789943  0.851987  0.947308  \n",
       "6   0.808478  0.746968  0.962865  \n",
       "7   0.821749  0.678378  0.961545  \n",
       "8   0.822607  0.676276  0.962301  \n",
       "9   0.527830  0.671079  0.928591  \n",
       "10  0.527659  0.670910  0.928608  \n",
       "11  0.834048  0.627502  0.963127  \n",
       "12  0.843258  0.599094  0.963100  \n",
       "13  0.842172  0.598030  0.962914  \n",
       "14  0.819976  0.558526  0.937437  \n",
       "15  0.819976  0.558515  0.937440  \n",
       "16  0.819690  0.558070  0.937440  \n",
       "17  0.818546  0.557562  0.937068  \n",
       "18  0.818660  0.556707  0.937122  \n",
       "19  0.818603  0.556690  0.937123  \n",
       "20  0.818546  0.556078  0.937128  \n",
       "21  0.818660  0.555789  0.937150  \n",
       "22  0.827470  0.551647  0.938846  \n",
       "23  0.827527  0.551590  0.938859  \n",
       "24  0.828042  0.550726  0.938884  \n",
       "25  0.827985  0.550572  0.938878  \n",
       "26  0.828156  0.550498  0.938881  \n",
       "27  0.827241  0.550193  0.938625  \n",
       "28  0.811624  0.539170  0.934627  \n",
       "29  0.374807  0.533746  0.918991  \n",
       "30  0.813683  0.532385  0.934880  \n",
       "31  0.813855  0.532039  0.934895  \n",
       "32  0.333448  0.488211  0.924636  \n",
       "33  0.333448  0.488211  0.924636  \n",
       "34  0.323494  0.477276  0.924119  \n",
       "35  0.815686  0.475316  0.924706  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96965c",
   "metadata": {},
   "source": [
    "Based on the results for dataset 3, `Random Forest` with no sampling techniques and `MinMaxScaler` gave the best F1-score of 0.88 and one of the best ROC AUC. The model with the best ROC AUC is `LinearSVC` with `ROS (Random Over Sampling)` and a `StandardScaler` with a value of 0.937 (row 14 in the results table).\n",
    "\n",
    "This shows that the classification problem is a non-linear one, as LinearSVC and Logistic Regression both struggle to produce a high enough F1-score, while Random Forest did produce a high F1-score, which is known for handling non-linear relationships well.\n",
    "\n",
    "The highest performing `Logistic Regression` model is without any scaling or sampling techniques, with an F1-score of 0.67 and a ROC AUC of 0.93.\n",
    "\n",
    "The highest performing `LinearSVC` is with `Random Over Sampling (ROS)` and the data being scaled using the `StandardScaler`. These techniques increased the F1-score from 0.49 to 0.48 and the ROC AUC from 0.92 to 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d024f98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: test_size=0.2, random_state=42, stratify=y\n",
      "Train shape: (227845, 30), Test shape: (56962, 30)\n",
      "Test label distribution:\n",
      "Class\n",
      "0    0.99828\n",
      "1    0.00172\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "\n",
      "=== Evaluating Model: Logistic Regression ===\n",
      "\n",
      "=== Evaluating Model: Random Forest ===\n",
      "\n",
      "=== Evaluating Model: Linear SVC ===\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "\n",
      "=== Pipeline Execution Complete ===\n"
     ]
    }
   ],
   "source": [
    "#dataset 2\n",
    "dbtable = pd.read_csv(\"creditcard.csv\")\n",
    "X = dbtable.drop(\"Class\", axis=1).copy()\n",
    "y = dbtable[\"Class\"]\n",
    "numerical_cols_actual = [col for col in X.columns if X[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "all_results2 = model_comparison(models_to_test, scalers, samplers, X, y, numerical_cols_actual, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb4baaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.874317</td>\n",
       "      <td>0.963027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.874317</td>\n",
       "      <td>0.963027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.963039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.848168</td>\n",
       "      <td>0.968451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.962822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.962821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.962804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999456</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.839378</td>\n",
       "      <td>0.963982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999421</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.964423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.948542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.716763</td>\n",
       "      <td>0.960549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.942616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999087</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.962518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999087</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.943127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.979143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.996787</td>\n",
       "      <td>0.334630</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.484507</td>\n",
       "      <td>0.977924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.988308</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.209026</td>\n",
       "      <td>0.976047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.985587</td>\n",
       "      <td>0.097887</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.176530</td>\n",
       "      <td>0.978518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.982146</td>\n",
       "      <td>0.081130</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.148954</td>\n",
       "      <td>0.971696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.979548</td>\n",
       "      <td>0.072173</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.133829</td>\n",
       "      <td>0.975129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.979478</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.133432</td>\n",
       "      <td>0.974638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.979618</td>\n",
       "      <td>0.071716</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.132935</td>\n",
       "      <td>0.971147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.977353</td>\n",
       "      <td>0.065598</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.974866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.977283</td>\n",
       "      <td>0.064774</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.120924</td>\n",
       "      <td>0.974963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.976651</td>\n",
       "      <td>0.063121</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.118037</td>\n",
       "      <td>0.975251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.976528</td>\n",
       "      <td>0.062809</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.117492</td>\n",
       "      <td>0.975905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.975668</td>\n",
       "      <td>0.061308</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.972004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.974106</td>\n",
       "      <td>0.057803</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.108761</td>\n",
       "      <td>0.970843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.969313</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.093361</td>\n",
       "      <td>0.972548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.964116</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.080935</td>\n",
       "      <td>0.977698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.964116</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.080935</td>\n",
       "      <td>0.977697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.964099</td>\n",
       "      <td>0.042313</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.080899</td>\n",
       "      <td>0.977698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.962326</td>\n",
       "      <td>0.040395</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.077386</td>\n",
       "      <td>0.974489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.960219</td>\n",
       "      <td>0.038330</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.073590</td>\n",
       "      <td>0.976126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.959412</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.072231</td>\n",
       "      <td>0.974876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.957674</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.069471</td>\n",
       "      <td>0.975351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model          Scaler Sampler  Accuracy  Precision  \\\n",
       "0         Random Forest            None    None  0.999596   0.941176   \n",
       "1         Random Forest  StandardScaler    None  0.999596   0.941176   \n",
       "2         Random Forest    MinMaxScaler    None  0.999561   0.929412   \n",
       "3         Random Forest  StandardScaler   SMOTE  0.999491   0.870968   \n",
       "4         Random Forest  StandardScaler     ROS  0.999526   0.949367   \n",
       "5         Random Forest            None     ROS  0.999526   0.949367   \n",
       "6         Random Forest    MinMaxScaler     ROS  0.999526   0.949367   \n",
       "7         Random Forest    MinMaxScaler   SMOTE  0.999456   0.852632   \n",
       "8         Random Forest            None   SMOTE  0.999421   0.835052   \n",
       "9   Logistic Regression            None    None  0.999157   0.828947   \n",
       "10  Logistic Regression  StandardScaler    None  0.999140   0.826667   \n",
       "11           Linear SVC            None    None  0.999122   0.807692   \n",
       "12           Linear SVC    MinMaxScaler    None  0.999087   0.828571   \n",
       "13           Linear SVC  StandardScaler    None  0.999087   0.828571   \n",
       "14  Logistic Regression    MinMaxScaler    None  0.998894   0.786885   \n",
       "15  Logistic Regression    MinMaxScaler     RUS  0.996787   0.334630   \n",
       "16  Logistic Regression            None   SMOTE  0.988308   0.118280   \n",
       "17           Linear SVC    MinMaxScaler     RUS  0.985587   0.097887   \n",
       "18           Linear SVC            None   SMOTE  0.982146   0.081130   \n",
       "19           Linear SVC    MinMaxScaler     ROS  0.979548   0.072173   \n",
       "20           Linear SVC  StandardScaler     ROS  0.979478   0.071942   \n",
       "21           Linear SVC            None     ROS  0.979618   0.071716   \n",
       "22  Logistic Regression    MinMaxScaler     ROS  0.977353   0.065598   \n",
       "23           Linear SVC  StandardScaler   SMOTE  0.977283   0.064774   \n",
       "24  Logistic Regression    MinMaxScaler   SMOTE  0.976651   0.063121   \n",
       "25           Linear SVC    MinMaxScaler   SMOTE  0.976528   0.062809   \n",
       "26  Logistic Regression  StandardScaler     ROS  0.975668   0.061308   \n",
       "27  Logistic Regression  StandardScaler   SMOTE  0.974106   0.057803   \n",
       "28  Logistic Regression            None     ROS  0.969313   0.049180   \n",
       "29        Random Forest    MinMaxScaler     RUS  0.964116   0.042333   \n",
       "30        Random Forest  StandardScaler     RUS  0.964116   0.042333   \n",
       "31        Random Forest            None     RUS  0.964099   0.042313   \n",
       "32           Linear SVC            None     RUS  0.962326   0.040395   \n",
       "33  Logistic Regression  StandardScaler     RUS  0.960219   0.038330   \n",
       "34           Linear SVC  StandardScaler     RUS  0.959412   0.037594   \n",
       "35  Logistic Regression            None     RUS  0.957674   0.036101   \n",
       "\n",
       "      Recall  F1 Score   ROC AUC  \n",
       "0   0.816327  0.874317  0.963027  \n",
       "1   0.816327  0.874317  0.963027  \n",
       "2   0.806122  0.863388  0.963039  \n",
       "3   0.826531  0.848168  0.968451  \n",
       "4   0.765306  0.847458  0.962822  \n",
       "5   0.765306  0.847458  0.962821  \n",
       "6   0.765306  0.847458  0.962804  \n",
       "7   0.826531  0.839378  0.963982  \n",
       "8   0.826531  0.830769  0.964423  \n",
       "9   0.642857  0.724138  0.948542  \n",
       "10  0.632653  0.716763  0.960549  \n",
       "11  0.642857  0.715909  0.942616  \n",
       "12  0.591837  0.690476  0.962518  \n",
       "13  0.591837  0.690476  0.943127  \n",
       "14  0.489796  0.603774  0.979143  \n",
       "15  0.877551  0.484507  0.977924  \n",
       "16  0.897959  0.209026  0.976047  \n",
       "17  0.897959  0.176530  0.978518  \n",
       "18  0.908163  0.148954  0.971696  \n",
       "19  0.918367  0.133829  0.975129  \n",
       "20  0.918367  0.133432  0.974638  \n",
       "21  0.908163  0.132935  0.971147  \n",
       "22  0.918367  0.122449  0.974866  \n",
       "23  0.908163  0.120924  0.974963  \n",
       "24  0.908163  0.118037  0.975251  \n",
       "25  0.908163  0.117492  0.975905  \n",
       "26  0.918367  0.114943  0.972004  \n",
       "27  0.918367  0.108761  0.970843  \n",
       "28  0.918367  0.093361  0.972548  \n",
       "29  0.918367  0.080935  0.977698  \n",
       "30  0.918367  0.080935  0.977697  \n",
       "31  0.918367  0.080899  0.977698  \n",
       "32  0.918367  0.077386  0.974489  \n",
       "33  0.918367  0.073590  0.976126  \n",
       "34  0.918367  0.072231  0.974876  \n",
       "35  0.918367  0.069471  0.975351  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841f479",
   "metadata": {},
   "source": [
    "For Dataset 2, the highest performer was also the baseline `Random Forest` with no sampling techniques or data scaling applied to it. The baseline gave an F1-score of 0.82 with a ROC AUC of 0.96. The model that gave the highest ROC AUC however, is the Logistic Regression with no sampling techniques but the data is scaled using the `MinMaxScaler` with a ROC AUC  0.98 (row with the index of 14)\n",
    "\n",
    "The highest performing `Logistic Regression` is also the baseline with no sampling techniques or data scaling applied to it. This gave an F1-score of 0.72 and a ROC AUC of 0.95.\n",
    "\n",
    "The highest performing `LinearSVC` is also the baseline. It gave an F1-score of 0.72 and a ROC AUC of 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff7d3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: test_size=0.2, random_state=42, stratify=y\n",
      "Train shape: (454904, 29), Test shape: (113726, 29)\n",
      "Test label distribution:\n",
      "Class\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "\n",
      "=== Evaluating Model: Logistic Regression ===\n",
      "\n",
      "=== Evaluating Model: Random Forest ===\n",
      "\n",
      "=== Evaluating Model: Linear SVC ===\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "Info: Used decision_function() for ROC AUC (Linear SVC).\n",
      "\n",
      "=== Pipeline Execution Complete ===\n"
     ]
    }
   ],
   "source": [
    "#dataset 1\n",
    "df = pd.read_csv(\"creditcard_2023.csv\")\n",
    "\n",
    "X = df.drop(['id', 'Class'], axis = 1)\n",
    "y = df['Class']\n",
    "numerical_cols_actual = [col for col in X.columns if X[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "all_results1 = model_comparison(models_to_test, scalers, samplers, X, y, numerical_cols_actual, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "187e78d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.964977</td>\n",
       "      <td>0.977170</td>\n",
       "      <td>0.952201</td>\n",
       "      <td>0.964524</td>\n",
       "      <td>0.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.964977</td>\n",
       "      <td>0.977170</td>\n",
       "      <td>0.952201</td>\n",
       "      <td>0.964524</td>\n",
       "      <td>0.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.964977</td>\n",
       "      <td>0.977170</td>\n",
       "      <td>0.952201</td>\n",
       "      <td>0.964524</td>\n",
       "      <td>0.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.964977</td>\n",
       "      <td>0.977170</td>\n",
       "      <td>0.952201</td>\n",
       "      <td>0.964524</td>\n",
       "      <td>0.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.964810</td>\n",
       "      <td>0.976956</td>\n",
       "      <td>0.952078</td>\n",
       "      <td>0.964356</td>\n",
       "      <td>0.993478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.964810</td>\n",
       "      <td>0.976956</td>\n",
       "      <td>0.952078</td>\n",
       "      <td>0.964356</td>\n",
       "      <td>0.993478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.964810</td>\n",
       "      <td>0.976956</td>\n",
       "      <td>0.952078</td>\n",
       "      <td>0.964356</td>\n",
       "      <td>0.993478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.964327</td>\n",
       "      <td>0.977917</td>\n",
       "      <td>0.950108</td>\n",
       "      <td>0.963812</td>\n",
       "      <td>0.993377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.980153</td>\n",
       "      <td>0.945817</td>\n",
       "      <td>0.962679</td>\n",
       "      <td>0.993314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.980153</td>\n",
       "      <td>0.945817</td>\n",
       "      <td>0.962679</td>\n",
       "      <td>0.993314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.980153</td>\n",
       "      <td>0.945817</td>\n",
       "      <td>0.962679</td>\n",
       "      <td>0.993314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.980153</td>\n",
       "      <td>0.945817</td>\n",
       "      <td>0.962679</td>\n",
       "      <td>0.993314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.963210</td>\n",
       "      <td>0.980271</td>\n",
       "      <td>0.945448</td>\n",
       "      <td>0.962545</td>\n",
       "      <td>0.993309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.963210</td>\n",
       "      <td>0.980271</td>\n",
       "      <td>0.945448</td>\n",
       "      <td>0.962545</td>\n",
       "      <td>0.993309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.963210</td>\n",
       "      <td>0.980271</td>\n",
       "      <td>0.945448</td>\n",
       "      <td>0.962545</td>\n",
       "      <td>0.993309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.963210</td>\n",
       "      <td>0.980271</td>\n",
       "      <td>0.945448</td>\n",
       "      <td>0.962545</td>\n",
       "      <td>0.993309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.961629</td>\n",
       "      <td>0.993196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.961629</td>\n",
       "      <td>0.993196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.961629</td>\n",
       "      <td>0.993196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.962339</td>\n",
       "      <td>0.980341</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.961620</td>\n",
       "      <td>0.993197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>RUS</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.941702</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.992522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>None</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.941702</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.992522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.941702</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.992522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.941702</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.992522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model          Scaler Sampler  Accuracy  Precision  \\\n",
       "0         Random Forest            None    None  0.999833   0.999666   \n",
       "1         Random Forest            None   SMOTE  0.999833   0.999666   \n",
       "2         Random Forest            None     ROS  0.999833   0.999666   \n",
       "3         Random Forest  StandardScaler    None  0.999833   0.999666   \n",
       "4         Random Forest  StandardScaler   SMOTE  0.999833   0.999666   \n",
       "5         Random Forest  StandardScaler     ROS  0.999833   0.999666   \n",
       "6         Random Forest    MinMaxScaler    None  0.999833   0.999666   \n",
       "7         Random Forest    MinMaxScaler   SMOTE  0.999833   0.999666   \n",
       "8         Random Forest    MinMaxScaler     ROS  0.999833   0.999666   \n",
       "9         Random Forest    MinMaxScaler     RUS  0.999807   0.999613   \n",
       "10        Random Forest            None     RUS  0.999807   0.999613   \n",
       "11        Random Forest  StandardScaler     RUS  0.999807   0.999613   \n",
       "12  Logistic Regression  StandardScaler    None  0.964977   0.977170   \n",
       "13  Logistic Regression  StandardScaler   SMOTE  0.964977   0.977170   \n",
       "14  Logistic Regression  StandardScaler     RUS  0.964977   0.977170   \n",
       "15  Logistic Regression  StandardScaler     ROS  0.964977   0.977170   \n",
       "16  Logistic Regression            None    None  0.964810   0.976956   \n",
       "17  Logistic Regression            None   SMOTE  0.964810   0.976956   \n",
       "18  Logistic Regression            None     ROS  0.964810   0.976956   \n",
       "19  Logistic Regression            None     RUS  0.964327   0.977917   \n",
       "20           Linear SVC  StandardScaler    None  0.963333   0.980153   \n",
       "21           Linear SVC  StandardScaler   SMOTE  0.963333   0.980153   \n",
       "22           Linear SVC  StandardScaler     ROS  0.963333   0.980153   \n",
       "23           Linear SVC  StandardScaler     RUS  0.963333   0.980153   \n",
       "24           Linear SVC            None    None  0.963210   0.980271   \n",
       "25           Linear SVC            None   SMOTE  0.963210   0.980271   \n",
       "26           Linear SVC            None     ROS  0.963210   0.980271   \n",
       "27           Linear SVC            None     RUS  0.963210   0.980271   \n",
       "28           Linear SVC    MinMaxScaler    None  0.962348   0.980358   \n",
       "29           Linear SVC    MinMaxScaler   SMOTE  0.962348   0.980358   \n",
       "30           Linear SVC    MinMaxScaler     ROS  0.962348   0.980358   \n",
       "31           Linear SVC    MinMaxScaler     RUS  0.962339   0.980341   \n",
       "32  Logistic Regression    MinMaxScaler     RUS  0.961618   0.980769   \n",
       "33  Logistic Regression    MinMaxScaler    None  0.961618   0.980769   \n",
       "34  Logistic Regression    MinMaxScaler   SMOTE  0.961618   0.980769   \n",
       "35  Logistic Regression    MinMaxScaler     ROS  0.961618   0.980769   \n",
       "\n",
       "      Recall  F1 Score   ROC AUC  \n",
       "0   1.000000  0.999833  0.999990  \n",
       "1   1.000000  0.999833  0.999990  \n",
       "2   1.000000  0.999833  0.999990  \n",
       "3   1.000000  0.999833  0.999990  \n",
       "4   1.000000  0.999833  0.999990  \n",
       "5   1.000000  0.999833  0.999990  \n",
       "6   1.000000  0.999833  0.999990  \n",
       "7   1.000000  0.999833  0.999990  \n",
       "8   1.000000  0.999833  0.999990  \n",
       "9   1.000000  0.999807  0.999990  \n",
       "10  1.000000  0.999807  0.999990  \n",
       "11  1.000000  0.999807  0.999990  \n",
       "12  0.952201  0.964524  0.993500  \n",
       "13  0.952201  0.964524  0.993500  \n",
       "14  0.952201  0.964524  0.993500  \n",
       "15  0.952201  0.964524  0.993500  \n",
       "16  0.952078  0.964356  0.993478  \n",
       "17  0.952078  0.964356  0.993478  \n",
       "18  0.952078  0.964356  0.993478  \n",
       "19  0.950108  0.963812  0.993377  \n",
       "20  0.945817  0.962679  0.993314  \n",
       "21  0.945817  0.962679  0.993314  \n",
       "22  0.945817  0.962679  0.993314  \n",
       "23  0.945817  0.962679  0.993314  \n",
       "24  0.945448  0.962545  0.993309  \n",
       "25  0.945448  0.962545  0.993309  \n",
       "26  0.945448  0.962545  0.993309  \n",
       "27  0.945448  0.962545  0.993309  \n",
       "28  0.943601  0.961629  0.993196  \n",
       "29  0.943601  0.961629  0.993196  \n",
       "30  0.943601  0.961629  0.993196  \n",
       "31  0.943601  0.961620  0.993197  \n",
       "32  0.941702  0.960838  0.992522  \n",
       "33  0.941702  0.960838  0.992522  \n",
       "34  0.941702  0.960838  0.992522  \n",
       "35  0.941702  0.960838  0.992522  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c925a7f",
   "metadata": {},
   "source": [
    "Dataset 1 seems synthetic or manipulated as it was balanced, which is unrealistic for datacard fraud. For this dataset, the highest performer is `Random Forest` with no sampling or scaling techniques, with an F1-score of 0.99 and a ROC AUC of 0.99.\n",
    "\n",
    "The highest performing `Logistic Regression` is with no sampling technique but with the `StandardScaler`. This increased the F1-score from 0.964356 to 0.964524, a very small increase of 0.000168. The ROC AUC also increased very slightly.\n",
    "\n",
    "The highest performing `LinearSVC` is also with no sampling technique but with the `StandardScaler`. It got an F1-score of 0.964 and a ROC AUC of 0.9934. Compared to the baseline `LinearSVC` with no techniques, it also had very minute changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbea82",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The experimental investigation across three distinct datasets provided data on the comparative performance of Random Forest, Logistic Regression, and Linear SVC for credit card fraud detection. The impact of feature scaling and class imbalance sampling techniques was also assessed. The primary evaluation metric was the F1-score, due to its relevance in imbalanced classification, with the ROC AUC score providing a complementary measure of model discriminative ability.\n",
    "\n",
    "On Dataset 3, which presented challenges for linear models, Random Forest achieved the highest F1-score. The configuration using `MinMaxScaler` without sampling yielded an `F1-score` of approximately `0.879`, while the baseline Random Forest (no preprocessing) also performed well with an F1-score of 0.877. This suggests Random Forest's capacity to model the data effectively. In contrast, Logistic Regression and Linear SVC produced lower F1-scores on this dataset. Logistic Regression's best F1-score was 0.671 without preprocessing, and Linear SVC's was approximately 0.559 with StandardScaler and RandomOverSampler. These results indicate that a linear function may not adequately approximate the decision boundary for this dataset. Sampling techniques like SMOTE and RUS, when applied to Random Forest, generally increased recall but decreased precision, leading to lower F1-scores. This suggests that the introduced or remaining samples might have adversely affected the model's precision-recall balance.\n",
    "\n",
    "For Dataset 2, Random Forest again achieved the highest F1-score (0.874) in its baseline configuration. This dataset also favored the non-linear modeling capabilities of Random Forest. Linear models showed some response to preprocessing; for instance, Logistic Regression achieved its highest ROC AUC with MinMaxScaler, though its F1-score did not surpass its baseline. Linear SVC performed best with its baseline setup. Similar to Dataset 3, common sampling techniques generally did not improve, and often reduced, the F1-scores for all models on Dataset 2, primarily through a negative impact on precision.\n",
    "\n",
    "Dataset 1 appeared to be either balanced or synthetically generated, leading to high performance across all models. Random Forest achieved high F1-scores with minimal preprocessing. Logistic Regression and Linear SVC also performed well, with minor, if any, improvements observed when StandardScaler was applied. Given the dataset's characteristics, sampling techniques offered no discernible benefits.\n",
    "\n",
    "Across all datasets, several patterns were observed. Random Forest consistently produced high F1-scores, often with no specific scaling or sampling. This indicates its ability to handle complex data structures. Linear models, Logistic Regression and Linear SVC, were more influenced by dataset characteristics. Their performance was lower than Random Forest on Dataset 3. Scaling sometimes offered slight benefits to their ROC AUC scores or specific F1 components but did not enable them to match Random Forest's F1-scores where non-linearity was a factor. The investigated sampling techniques showed mixed and often unfavorable results for F1-score improvement. While recall for the minority class was often increased, this typically occurred at the expense of precision. Scalers had minimal impact on Random Forest, as anticipated, while their effect on linear models was inconsistent and generally modest for the F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb75796",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In summary, this empirical evaluation indicates that Random Forest generally achieved the highest F1-scores among the three algorithms tested for credit card fraud detection, particularly on the imbalanced datasets with apparent non-linear characteristics (Datasets 2 and 3). Its performance often peaked with minimal preprocessing. Logistic Regression and Linear SVC, while offering different modeling approaches, did not match Random Forest's F1-scores on the more complex datasets. Feature scaling provided inconsistent and generally minor improvements to F1-scores for these linear models. The application of the selected sampling techniques did not consistently lead to better F1-scores; often, recall for the minority class increased while precision decreased, resulting in a net negative or negligible impact on the F1-score. These findings suggest that for the datasets and configurations tested, an algorithm like Random Forest, capable of addressing non-linear relationships, was more effective than the linear models, and that standard preprocessing steps like scaling and sampling require careful, dataset-specific validation to ensure they provide a net benefit to the chosen performance metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
